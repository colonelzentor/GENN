{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import example_pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jenn import JENN\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "from pyDOE2 import lhs, fullfact\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Two-Dimensional Rastrigin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be to teach a neural network to predict the Rastrigin function, which is a challenging multi-modal function that somewhat ressembles an egg-crate. Mathematically, the Rastrigin function is defined as: \n",
    "\n",
    "$$f(x_1 \\cdots x_n) = 10n + \\sum_{i=1}^n (x_i^2 -10cos(2\\pi x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Training and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step, let's define the test function we will be working with, taken to be a simple sinusoidal function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jenn.tests.test_problems import rastrigin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define the domain over which we will collect synthetic training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain\n",
    "lb = -1.\n",
    "ub = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now generate some synthetic data. GENN uses numpy and, therefore, the training data must be provided as numpy arrays where $m$ is the number of samples in the data set, $n_x$ is the number of inputs, and $n_y$ is the number outputs: \n",
    "\n",
    "* $X$ = sample inputs of shape $(m, n_x)$\n",
    "* $Y$ = sample outputs of shape $(m, n_y)$\n",
    "* $J$ = sample jacobian of shape $(m, n_x, n_y)$\n",
    "\n",
    "Here's an example below (using pyDOE2): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data \n",
    "levels = 9\n",
    "X_train = fullfact([levels] * 2) / (levels - 1)  * (ub - lb) + lb\n",
    "Y_train, J_train = rastrigin(X_train)\n",
    "\n",
    "# Test Data \n",
    "X_test = lb + (ub - lb) * lhs(2, samples=75, criterion='maximin', iterations=100, random_state=5)\n",
    "Y_test, J_test = rastrigin(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initalize the model, where $deep \\ge 1$ and $wide \\ge 1$ are the number of layers in the neural network and the number of nodes per layer, respectively. Feel free to try different network architectures. The parameters $n_x \\ge 1$ and $n_y \\ge 1$ are the number of inputs and outputs, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep = 2\n",
    "wide = 24\n",
    "model = JENN(hidden_layer_sizes=[wide] * deep, activation='tanh',\n",
    "             num_epochs=1, max_iter=500,\n",
    "             learning_rate='backtracking', random_state=0, tol=1e-6,\n",
    "             learning_rate_init=0.05, alpha=0.001, gamma=1, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been initialized, we can train it. To do so, the user only needs to provide training data and tune the hyper-parameters that control the behavior of the training algorithm. They are listed here-under:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, J_train, is_normalize=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good practice to check the convergence history in order to see if we should keep training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.training_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Check Goodness of Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon convergence, we now check the goodness of fit using the test data we generated earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.goodness_fit(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Plot Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since this is a two-dimensional function, we can compare the contour plots of the predicted and true response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def contour_plot(model, X_train, X_test):\n",
    "    \"\"\"Make contour plots of 2D Rastrigin function and compare to Neural Net prediction\"\"\"\n",
    "\n",
    "    if model.gamma == 1: \n",
    "        title = 'genn'\n",
    "    else: \n",
    "        title = 'nn'\n",
    "    # Domain\n",
    "    lb = -1.\n",
    "    ub = 1.5\n",
    "    m = 100\n",
    "    x1 = np.linspace(lb, ub, m)\n",
    "    x2 = np.linspace(lb, ub, m)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "    # True response\n",
    "    pi = np.pi\n",
    "    Y_true = np.zeros((m, m))  \n",
    "    for i in range(m): \n",
    "        for j in range(m): \n",
    "            x = np.array([[X1[i, j], X2[i, j]]])\n",
    "            Y_true[i, j] = rastrigin(x)[0]\n",
    "        \n",
    "    # Predicted response\n",
    "    Y_pred = np.zeros((m, m))\n",
    "    for i in range(0, m):\n",
    "        for j in range(0, m):\n",
    "            x = np.array([[X1[i, j], X2[i, j]]])\n",
    "            Y_pred[i, j] = model.predict(x)\n",
    "\n",
    "    # Prepare to plot\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    spec = gridspec.GridSpec(ncols=2, nrows=1, wspace=0)\n",
    "\n",
    "    # Plot Truth model\n",
    "    ax1 = fig.add_subplot(spec[0, 0])\n",
    "    ax1.contour(X1, X2, Y_true, 20, cmap='RdGy')\n",
    "    anno_opts = dict(xy=(0.5, 1.075), xycoords='axes fraction', va='center', ha='center')\n",
    "    ax1.annotate('True', **anno_opts)\n",
    "    anno_opts = dict(xy=(-0.075, 0.5), xycoords='axes fraction', va='center', ha='center')\n",
    "    ax1.annotate('X2', **anno_opts)\n",
    "    anno_opts = dict(xy=(0.5, -0.05), xycoords='axes fraction', va='center', ha='center')\n",
    "    ax1.annotate('X1', **anno_opts)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.scatter(X_train[0, :], X_train[1, :], s=5)\n",
    "    ax1.set_xlim(lb, ub)\n",
    "    ax1.set_ylim(lb, ub)\n",
    "    ax1.plot(X_train[:, 0], X_train[:, 1], 'k.', label='train')\n",
    "    ax1.plot(X_test[:, 0], X_test[:, 1], 'r+', label='test')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot prediction with gradient enhancement\n",
    "    ax2 = fig.add_subplot(spec[0, 1])\n",
    "    ax2.contour(X1, X2, Y_pred, 20, cmap='RdGy')\n",
    "    anno_opts = dict(xy=(0.5, 1.075), xycoords='axes fraction', va='center', ha='center')\n",
    "    ax2.annotate(title, **anno_opts)\n",
    "    anno_opts = dict(xy=(0.5, -0.05), xycoords='axes fraction', va='center', ha='center')\n",
    "    ax2.annotate('X1', **anno_opts)\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot(model, X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, good agreement is obtained :-) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Using the exact same inputs, except for $\\gamma = 0$, let's compare to regular neural networks:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gamma = 0\n",
    "model.fit(X_train, Y_train, is_normalize=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot(model, X_train, X_test)\n",
    "model.goodness_fit(X_test, Y_test)\n",
    "history = model.training_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Everything else being equal, JENN tends to improve the accuracy of the prediction compared to standard Multi-Layer Perceptrons (MLP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
